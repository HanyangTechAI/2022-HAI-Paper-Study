22이지명 HAI 과제

-----

<br>

# 2주차 논문


Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., ... & Hassabis, D. (2016). Mastering the game of Go with deep neural networks and tree search. nature, 529(7587), 484-489.

<br>

---

<br>

# 논문 요약

<br>

### Abstract
이 연구에서는 tree search 없이 neural network의 supervision/reinforecement learning을 통해 board state를 평가하는 방법론을 제안하여 기존 sota 바둑 인공지능의 수준을 달성하였다.

추가로, 학습된 네트워크를 tree search에 적용하는 방법론을 제안하였고, 이를 통해 인간 프로기사의 실력을 뛰어넘는 수준의 인공지능을 구현하였다.

### Pollicy Network

- SL policy $p_\sigma$는 인간 기보 데이터를 사용하여 다음 수를 예측하도록 학습시킨다.
- $a \sim p_\sigma$ 인 selfplay 데이터로 RL policy $p_\rho$를 학습시킨다.

### Value Network
- optimal value function $v^{\star}(s)$를 target으로, 
$v^{\star}(s) \approx v^{p_\rho}(s) \approx v_\theta (s)$의 근사를 통해 $v_\theta$를 학습시킨다.

### Tree Search
- Monte Carlo Tree Search를 기반으로, MCTS의 selection, expansion, rollout 단계에 network를 적용한다.
  - selection : 선택에 사용되는 $UCT \equiv Q(s,a) + U(s,a)$에서 예측된 선택 확률을 반영한 $PUCT \equiv Q(s,a) + p(a \vert s)U(s,a)$를 사용한다.
  - expansion : 노드를 확장할때 tree policy network를 통해 $p(a \vert s)$을 계산하여 저장한다(PUCT 계산에 활용).
  - rollout : random rollout 대신 rollout policy $p_\pi$를 사용하여 $a \sim p_\pi$로 simulation하고, $v_\theta (s)$의 예측값과 섞어 $Q(s, a)$를 평가한다.

### Evaulation

- lookahead search 없이 policy network만을 사용하여 sota 바둑 인공지능의 수준을 달성
- 인간 프로기사를 상대로 Elo rating 200 이상의 우위(승률 79%이상)를 달성

------


