# HAI 논문 스터디 3주차

### Mastering the game of Go with deep neural networks and tree search

모든 세계급의 바둑 기사들을 무찌른 알파고의 기반이 되는 논문



## 배경

바둑은 인공지능에 있어서 수많은 어려움을 겪게 한 게임이었음

- 엄청 힘든 의사 결정 과정
- 엄청난 크기의 탐색 공간
- 직접 접근하기에는 너무 찾기 힘든 optimal solution



이를 해결하기 위해 보드의 상태를 평가하는 value network와 수를 도출하는 policy network 사용하는 방법 제안 -> 탐색 없이 network만으로 기존 프로그램 급의 성능이 나옴



이 value-policy network를 Monte Carlo simulation에 적용해보기로 함 -> 엄청난 성능 탄생



## 주요 기술 1 - Networks

Policy network : 모든 움직임들에 대한 확률 분포를 결과로 출력

Value network : 어떤 수에 대한 예상된 결과를 scalar로 출력



### RL Policy Network

- init : 기존에 있는 정보들을 활용한 SL Policy Network의 결과 사용

- 과정 : 이전 확률 분포를 활용하여 self-play를 계속 진행

- 최적화 : stochastic gradient ascent 활용



### Value Network

이론상 perfect play에서의 optimal value가 있지만, perfect play policy를 알 수 없으므로, 

RL policy의 결과물을 대신 사용함



- 과정 : RL policy로 얻은 확률 분포를 활용하여 self-play 진행 -> stochastic gradient descent를 활용하여 regression 진행

- overfitting 해결 방법 : 각각 다른 경기로부터 만들어진 3천만개의 다른 정보들을 생성



## 주요 기술 2 - Monte Carlo Tree Search

selection - expansion - evaluation - backup의 과정을 거치면서 경우의 수들을 탐색



### Selection

Q: action value, N: visit count, P: prior probability

이 세가지 지표를 활용한 수식을 통해서 성능 함수를 사용, 더 높은 지표를 도출해내는 쪽 선택



### Expansion

leaf node에 일정 횟수 이상 방문 -> 다음 경우의 수를 expand 하는 작업

leaf position은 기존의 데이터를 바탕으로 한 SL policy로 초기화 되고, 각 과정의 결과들은 prior probability로 저장되어 계속 활용됨



### Evaluation & Backup

Evaluation은 value network와 무작위 rollout policy로 평가 됨

-> 어느정도 value network에 기반하지만, 랜덤성을 충분히 부여해서 정확성을 높임



- Backup : 이전 노드들의 Q(action value)와 N(visit count)가 업데이트 됨



### Completion

- 마지막 결정은 최다 N(visit count)으로 결정됨

-> 최대 Q(action value)를 고르는 것 보다 이상치에 덜 민감함



- 그냥 SL policy를 사용하는 것이 RL policy를 쓰는 것보다 알파고에서 성능이 좋았음

-> 인간은 어느정도 생각을 거친 수를 두지만, 아직 RL은 single best move만을 고려하기 때문



## 결론

이 논문을 바탕으로 한 Go program을 deep neural network와 tree search로 구현해냄.

AlphaGo는 결국 프로기사 수준에 도달했음.

이 엄청난 성공으로 다른 분야에도 인공지능 기술을 적용해서 충분한 성과를 낼 수 있을 것이라는 기대가 증폭됨.