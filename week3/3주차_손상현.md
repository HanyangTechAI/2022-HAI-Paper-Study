# 논문스터디 3주차 과제

## Mastering the game of Go with deep neural networks and tree search

### Abstract
- 강화학습의 value와 policy를 신경망을 사용해 평가, 도출하는 방법 제안
- Monte Carlo 시뮬레이션에 value, policy network를 적용하는 방법론 제안

### 신경망 구성 (강화학습)
- 입력값 `s`: 바둑판의 상태를 19x19x41 크기의 이미지처럼 취급한다.
- 두 역할의 신경망:
    - Policy Network: 현재 상태에서 최적 policy 확률 `P(a|s)` 출력
    - Value Network: 현재 상태 `s`에서 임의 action `a'`을 취했을 때의 outcome `v(s')` 을 출력함. (강화학습에서 reward `Q(s',a')`을 의미.)

- Policy Network: 두 종류를 가짐
    - Supervised Learning(SL) Policy Network: 온라인 유명 바둑서버에서 대국 기록을 다운받아 모든 state-action 쌍에 대해서 conv 신경망을 단순 학습. 이때 수를 두는 순서는 중요하지 않고 현재 state에만 영향 받는다. 그래서 sample을 모두 섞어도 무방하다.
    - Reinforcement Learning(RL) Policy Network: SL network와 동일 구조. RL network는 현재 RL network policy와 이전 iteration에서 사용했던 policy network 중에서 랜덤하게 하나를 뽑은 다음 이 둘끼리 서로 대국을 하게 한 후, 둘 중에서 현재 네트워크가 최종적으로 이기면 reward를 +1, 지면 -1을 줌.

- Value Network
    - 이론적으로 optimal value function이 존재할 수 있지만, 경우의 수가 너무 많으므로 perfect play policy를 알 수 없다.
    - 그래서 알 수 있는 strongest policy인 RL policy에 대한 value function을 target으로 두고 학습함.
    - 연속적인 두 개의 상태는 돌 한 개만 다른, 굉장히 밀접한 상관관계를 가짐. Reward target은 전체 game에 정의되므로 게임에 overfitting이 발생함.
    - 이 문제를 해결하기 위해 3천만개의 데이터를 RL policy network끼리 자가대국으로 만들어서 또 value network를 학습시켰다.
    - Target인 state-outcome 쌍 (s, z)와 predicted value의 차이는 MSE 손실함수로 정의함.

- 학습 파이프라인 요약
    1. 실제 기보 기록으로 rollout policy(3x3 판에서 국소적으로 빠르게 예측하는 네트워크), SL policy를 학습
    2. SL policy로부터 initalize해서 RL policy 학습
    3. RL policy로 value network를 학습

### Monte Carlo Tree Search

- 기본 동작:
    - selection: 현재 상태(root node)에서 leaf node에 도달할 때까지 자식 노드 선택
    - expansion: leaf node의 state에서 가능한 action들로 새로운 child node 생성
    - simulation(rollout): leaf node부터 terminal state까지 simulation 진행
    - backpropagation: root node로 거슬러 올라가며 node에 simulation 결과 반영

- Upper Confidence Bound:
    - 어떤 상태에서 action을 선택하는 기준은 두가지가 있다.
        1. exploration: 탐색이 적었던 action을 선택
        2. exploitation: value 기대값이 가장 높은 action 선택
    - 이 기준을 고려해서 여러 탐색 방법을 선택할 수 있다.
        - e-greedy
        - upper confidence bound
        - thompson sampling

- MCTS는 full search를 하지 않아도 되지만, 바둑 적용을 위해서는 결국 breadth와 depth를 줄이는 과정이 필요하다. AlphaGo는 결국 MCTS이며 탐색 공간을 줄이기 위해 policy network들을 학습하는 아키텍처를 만들어 성능을 끌어올린 것 이다.