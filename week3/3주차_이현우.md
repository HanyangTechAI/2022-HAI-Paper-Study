# HAI 논문 스터디 3주차
### Mastering the game of Go with deep neural networks and tree search


## Abstract
바둑은 인공지능에게 여러가지로 어려움
* 힘든 의사 결정
* 엄청난 탐색 공간
* 최적의 해결책을 찾기에 매우 복잡

상태를 평가하기 위한 value network와 수를 도출하기 위한 policy network를 사용하는 방법을 제한
$\Rightarrow$탐색 없이도 기존 sota 수준 달성

MCTS에 value, policy network를 적용하는 방법론 제안
$\Rightarrow$다른 바둑 프로그램에 대해선 99.8%의 승률을 달성하며 프로를 뛰어넘음

## Networks

* Policy network  
움직임에 대한 가능성을 출력

* Value network  
위치에 대해서 예상되는 결과를 출력

## Policy network
지도 학습과 강화 학습 두가지 방법을 통해 학습

지도학습: KGS Go Server의 3천만 가지의 위치에서 학습  
강화학습: 자기대국을 통해 모델을 학습

## Value network
이론적으로 perfect play에서의 optimal value function이 존재하지만 perfect play policy를 알수가 없음

따라서 strongest policy인 RL policy에 대한 value function을 타겟으로 학습함

## Monte Carlo Tree Search
총 4단계로 나누어짐

* selection  
cuttent state에서 시작해서 leaf node에 도달할 때까지 자식 노드를 선택

* expansion  
leaf node의 state에서 가능한 action들로 새로운 child node를 생성

* simulation  
leaf node로부터 terminal state까지 simulation을 진행

* backpropagation  
root node까지 거슬러 올라가며 node에 simulation 결과 반영

## Evaluation
Elo 레이팅은 체스나 바둑과 같은 분야에서 플레이어들의 실력을 표현하기 위해 만든 점수 측정 방식

기존의 바둑 프로그램들이 아마추어 정도였던것과 달리 알파고는 판후이, 즉 프로와 같은 점수를 받으며 이는 분산된 환경에서는 더 높아짐

Rollouts, Value network, Policy network 이 세개의 구성 요소 중 하나씩 제거하였을때 모두 Elo 점수가 떨어지며 이는 모든 구성 요소가 중요함을 보여줌

당연하게도 더 많은 컴퓨팅 자원이 있을때 알파고의 성능 또한 향상


## Conclusion
이 연구에서 뉴럴 네트워크와 트리 서치의 조합에 기반한 바둑 프로그램이 프로 기사 레벨에 달성함

다른 도메인에서도 인공지능을 활용하면 사람과 비슷한 성능을 달성할 수 있을 것이라는 희망을 줌

## Follow-up Studies
* AlphaGo Zero  
인간의 데이터와 게임 규칙과 같은 도메인 지식 없이도 오직 강화 학습만으로도 인간을 뛰어넘는 성능을 달성함

* AlphaZero  
다양한 까다로운 영역에서 인간을 뛰어넘는 성능을 달성하는 알파제로 알고리즘을 보여줌