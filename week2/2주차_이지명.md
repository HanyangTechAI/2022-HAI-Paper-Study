22이지명 HAI 과제

-----

<br>

# 2주차 논문


Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.

<br>

---

<br>

# 논문 요약

<br>

기존의 sequence 처리 모델들은 Recurrent 모델이나 Convolution 모델, 그리고 이를 활용한 Encoder-Decoder 구조가 지배적이었다. 이때 RNN 기반 모델은 이전 단계의 출력이 이후 단계의 입력으로 사용되기 때문에 연산을 병렬화하기 힘들다. 

attention mechanism은 RNN 기반 Encoder-Decoder 모델에서 문장 거리에 따라 발생하는 망각 문제를 효과적으로 해결한다.

이 논문에서는 기존의 Encoder-Decoder 구조에서 Recurrent 구조를 완전히 제거하고 Attention mechanism만을 사용한 transformer architecture를 제안한다. transformer의 구조는 연산의 병렬화를 가능하게 하였을 뿐더러, 입력 문장의 의미가 한정된 hidden state만으로 표현되어 발생하던 bottleneck 문제를 해결한다.

<br>

### Architecture

General
 - $Q, K, V$ 모두 동일한 sequence를 사용하는 self attention을 사용한다.
 - scaled dot prod attention을 사용한다.
 - 단일 attention이 아닌 multi-head attention을 사용한다.
 - attention - feed forward 블럭을 여러 층 쌓아 구성한다.
 - residual connection과 layer normalization을 사용한다.
 - 위치 정보를 반영하기 위해 positional encoding을 사용한다.

Encoder
- encoder output을 decoder의 각 층에서 attention의 $K, V$로 사용한다(normal attention).

Decoder
- self attention 이후 encoder attention을 사용한다.
- self attention의 경우 이후 토큰 정보를 미리 사용하지 못하도록 masking한다.

<br>

transformer는 machine translate 분야의 benchmark에서 sota 성능을 보여주었다.

------


