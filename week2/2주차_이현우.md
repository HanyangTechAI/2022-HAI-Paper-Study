# HAI 논문 스터디 2주차
### Attention Is All You Need


## Issues with recurrent models
* RNN은 O(sequence length) 스탭의 복잡도를 지님
* 병렬적으로 실행할 수 없으며 이는 매우 긴 시간을 필요함
* 메모리 한계로 큰 배치에서 학습을 진행할 수 없음


## Transformer
Encoder-Decoder 구조를 사용하여 기존에 방식이 지니던 문제를 해결

### Encoder

N개의 레이어로 구성되어 있음  
각각의 레이어는 Muti-Head Attention, Feed Forward, Add & Norm으로 구성  
positional encoding을 사용해 단어의 위치 정보를 반영  

#### Residual Connections
모델이 깊어짐에 따라 Vanishing Gradient와 같은 문제가 발생  
이를 해결하기 위해서 레이어의 출력과 입력을 합쳐 다음 레이어에 입력  
많은 레이어를 지나면서 중요한 정보를 잃어버리거나 왜곡하는 문제를 해결

#### LayerNorm
#### Scaled Dot Product Attention  


### Decoder
아래의 두 어탠션을 사용한 Masked Multi-Head Self-Attention, Encoder-Decoder Attention, Feed Forward 구성의 레이어가 반복됨  


#### Masked Multi-Head Self-Attention
나중에 나올 단어를 알게 되는 문제를 해결하기 위해서 이후에 있는 단어들을 마스킹하여 앞선 단어만 참고할 수 있도록 함

#### Encoder-Decoder Attention
query는 이전 레이어에서 key와 value들은 encoder의 출력으로부터 입력받음

마지막으로 Linear과 Softmax을 걸쳐 모델의 최종 출력이 됨
