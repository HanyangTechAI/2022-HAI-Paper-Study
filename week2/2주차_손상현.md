# 논문스터디 2주차 과제

## Attention is All You Need (Transformer)

### Problem of previous RNN

1. Time Dependency
RNN 모델은 time step에 따라 hidden state를 전달해 backward, forward pass를 한다. 이 방법은 다음과 같은 문제가 있다. 
    - O(sequence length) 복잡도의 forward & backward pass
    - Sequence가 길어진다면, 앞선 step에서는 gradient vanishing & explode의 문제가 있음.
    - Hidden state는 이전 time step의 hidden state에 의존함에 따라 병렬적으로 연산될 수 없다. 이는 매우 긴 학습 시간을 요구한다.

2. Bottleneck
    - Source sentence에 대해서 포착한 모든 정보를 encoder의 **마지막 hidden state만**을 사용해서 decoder로 전달하게 된다.
    - 제한된 크기의 state vector로 전달할 수 있는 정보의 양이 제한된다. 이는 bottleneck을 유발한다.

### What is Attention

Core Idea: Decoder의 매 스텝마다 source sentence의 특정 부분에 집중하기 위해 encoder와 직접적인 연결을 사용. (attention weight)

1. Decoder의 현재 step에서 `1: decoder의 hidden state`, `2: encoder의 모든 과거 hidden state`와 `3: attention weight`를 합성(dot product or else...)하여 attention score를 계산함.
2. Attention score에 softmax 연산을 취해서 현재 step의 attention score 확률 분포를 구함. 현재의 hidden state는 다음 단어 생성에 encoder의 몇번 step에 집중하고 있는지 파악 가능하다.

Attention의 계산
- Attention의 계산에는 query 벡터와 key 벡터, value 벡터가 주어짐.
    - query: each decoder hidden state
    - key: similiarity of selected decoder with all the encoder hidden states
    - value: all the encoder hidden states
- Query에 기반한 value들의 weighted(key) sum. 이는 모든 value들의 selective summary로 이해할 수 있다.

다음의 장점이 있다.
- Bottleneck 완화
- Gradient shortcut 제공
- 모델이 여러 언어의 단어간 alignment를 스스로 학습함.

### Positional Encoding

RNN을 통한 hidden state의 forward pass 대신, attention을 이용해 time dependent 정보를 처리한다. 단어들의 순서 정보를 알려주기 위해서는 positional encoding을 사용한다.

- Word Embedding in Self-Attention?
    - 전통적인 Embedding은 Input Embedding Matrix를 통해 one-hot 과 비슷한 형태를 취함.
    - RNN을 사용하지 않으려면 Embedding 과정에서 위치정보를 제공할 수 있어야함.
    - Positional Encoding은 Input Embedding Matrix와 같은 차원의 Positional Encoding Matrix를 **더해** 입력값으로 사용한다. 
    - 이제 입력값은 위치 정보를 함께 가지고 있게된다.


## Transformer

### Encoder

인코더는 입력값에서 feature를 추출하는 모델의 부분이다.

#### Self Attention and Feed Forward

Transformer는 encoding 단계의 word sequence 자체에 스스로 attention을 사용한다.
1. 세 종류의 가중치 `Query: Wq`, `Key: Wk`, `Value: Wv`가 있다.
2. 입력값 X로 Query, Key를 구해서 attention score를 구한다.
3. softmax를 취해 정규화한다.
4. key의 weighted sum을 구한다.
이 과정은 encoding 과정에서 positional information을 포함하고, 각 단어간 관련도를 feature에 반영한다.

여러개의 self attention 레이어를 가지고, 그 뒤에는 linear layer를 가진다.

#### Multi-Headed Self-Attention

Self-Attention을 수행할 때 사용하는 trick이다.
- 입력값 `X`로부터 h개의 서로 다른 Q, K, W를 구하도록 한다. (`Wq`, `Wk`, `Wv`가 각각 h개씩 있다는 뜻)
- h개의 서로 다른 attention concept을 학습하도록 만들어서 더 풍부한 정보를 학습할 수 있도록 하기 위함이다.
- 출력값은 단순 concatenate한다.
- 이때 attention의 입력값과 concatenate된 출력값의 dimension은 같도록 설정해야 한다.
- 이 뒤에 가중치 `Wo`를 곱해 최종 출력을 구한다.

#### Deeper Network

좋은건 다 가져다 썼다.

1. Residual Connection
2. Batch Normalization
3. Scaled Dot Product Attention: 
    attention에서 softmax에 의한 gradient vanishing을 해결하기 위함.

### Decoder

Decoder에서는 두 종류의 attention이 더 도입된다.

#### Encoder-Decoder Attention
트랜스포머에서는 마지막 인코더 레이어의 출력이 모든 디코더 레이어에 입력된다. 이때 attention을 사용한다. 구조는 앞선 Multi-Headed Self-Attention과 동일하다.

#### Masked Multi-Head Self-Attention
출력 문장에서는 앞쪽 단어가 나중에 나올 뒤쪽 단어를 참고하지 못하도록 계산에 마스크를 씌운다. 출력값은 항상 자신의 앞쪽의 단어만 참고할 수 있다. 구조는 앞선 Multi-Headed Self-Attention과 동일하다.

#### Finishing...

이 뒤에는 encoder와 마찬가지로 feedforward 네트워크가 오고, 마지막에 linear->softmax를 취해 최종 출력한다~