# <Abstract>

지금까지 complex recurrent or convolutiona neural network가 주류를 이루고 있음. 현 SOTA model도 attention mechanism으로 encoder와 decoder를 연결한 구조임. **우리는 Transformer라고 하는, attention mechanism만 가지는 새로운 모델을 제시.** 기존의 RNN model의 단점인 long-term dependency와 병렬화의 어려움을 해결함. 즉, 우리 모델은 SOTA model보다 뛰어난 성능을 지니면서, 병렬화가 더 쉽고, 학습 시간 또한 적게 걸림. 8개의 GPU에 3.5일 학습하였고, SOTA 성능을 보임.

# <Introduction>

LSTM, gated recurrent NN이 지금까지 좋은 성능을 보여 왔고, 최근의 연구들도 encoder-decoer architecture를 중심으로 진행되고 있음. 

Recurrent model은 step by step으로 input과 기존 step의 hidden state에 의해 현재의 hidden state를 만들어내는 방식으로 작동. 그 본질로 인해서 training 간에 parallelization이 힘들고, sequence length가 길어짐에 따라 제한적인 모습을 보임. 그로 인해, 최근의 성능 개선 연구는 모델 자체의 성능보다는 factorization trick이나 conditional computation 등 computation efficiency에 치중되어 진행됨. 

Attention mechanism은 I/O distance와 관계없이 dependency를 모델링한다는 점에서, compelling sequence modeling and transduction model의 주축으로 발전해옴. 때로는 recurrent network와도 결합되어 사용됨.

이 논문에서 recurrence를 배제하고, attention mechanism에만 의존해 만들어진 Transformer 구조를 도입해 기존 모델에 비해 병렬화와 성능 측면에서 강점을 가짐을 소개하고자 함.

transformer가 현재 각광받음은 물론, 논문 제목도 파격적으로 쓴 것이 대단하다는 느낌을 받음.